---
id: 002
title: RAG Chatbot Implementation Plan
stage: plan
date: 2025-12-10
surface: agent
model: claude-opus-4-5-20251101
feature: rag-chatbot
branch: master
user: system
command: /sp.plan
labels: ["planning", "architecture", "rag", "gemini", "qdrant", "fastapi", "react"]
links:
  spec: specs/rag-chatbot/spec.md
  ticket: null
  adr: null
  pr: null
files:
  - specs/rag-chatbot/plan.md
tests:
  - None (planning phase)
---

## Prompt

Based on the specification, create an implementation plan for the RAG chatbot integration.

PRIORITIES:
1. Set up backend FastAPI structure with environment configuration
2. Implement document ingestion pipeline (Docusaurus markdown ‚Üí Qdrant)
3. Build RAG query endpoint (retrieve + OpenAI generation)
4. Create React chat widget with text-selection detection
5. Integrate widget into Docusaurus theme
6. Add conversation history with Neon Postgres
7. Docker setup and deployment configs

STRUCTURE:
- Break into phases (backend, frontend, integration, deployment)
- Within each phase, create 2-5 minute tasks
- After each phase, include a checkpoint with:
  * Verification commands to run
  * Expected outputs
  * Manual tests to confirm everything works

Focus on:
- Minimal viable implementation first
- Clear file organization
- Testable increments

Output: Phased plan with micro-tasks and phase-end checkpoints.

## Response snapshot

Created comprehensive 6-phase implementation plan with 40+ micro-tasks (2-5 min each), organized by priority (P1 ‚Üí P2 ‚Üí P3).

**Architectural Decisions**:
1. **Gemini via OpenAI Agent SDK**: Use OpenAI SDK with Gemini-compatible endpoint for battle-tested patterns + free tier
2. **Single Qdrant Collection**: `docusaurus-book` (768 dims, Cosine) with metadata filtering for extensibility
3. **SSE Streaming**: Server-Sent Events for Gemini streaming (simpler than WebSockets, native browser support)
4. **Docusaurus Root Swizzling**: Non-invasive widget injection that survives upgrades
5. **Service-Oriented Backend**: Layered architecture (API ‚Üí Services ‚Üí Data) for testability

**6 Implementation Phases**:

1. **Phase 1: Backend Foundation & Ingestion (P1)** - 26 min
   - FastAPI project structure + dependencies
   - Qdrant client (768-dim collection, Cosine similarity)
   - Gemini embedding service (text-embedding-004)
   - Markdown chunking with heading hierarchy (H2/H3 boundaries, max 512 tokens)
   - Ingestion script (21 markdown files ‚Üí ~150-200 chunks)
   - **Checkpoint**: Verify all chunks indexed in Qdrant with proper metadata

2. **Phase 2: RAG Query Endpoint (P1)** - 21 min
   - Pydantic request/response schemas
   - LLM service with Gemini streaming via OpenAI SDK
   - RAG service orchestration (embedding ‚Üí retrieval ‚Üí generation)
   - `/chat` endpoint with SSE streaming + source citations
   - `/health` endpoint with dependency status
   - FastAPI app with CORS (localhost:3000 + Vercel prod URL)
   - **Checkpoint**: Verify streaming chat works, sources returned, health check passes

3. **Phase 3: Frontend Chat Widget (P1)** - 21 min
   - TypeScript API client with SSE streaming consumer
   - `useTextSelection` hook (mouseup detection, 8000 char limit)
   - `useChat` hook (message management, streaming state)
   - ChatWidget component (floating button, panel, selection badge)
   - CSS module with responsive design + dark mode support
   - Docusaurus Root swizzle integration
   - **Checkpoint**: Verify general chat, selection mode, source links, mobile responsive

4. **Phase 4: Conversation Persistence (P3)** - 8 min
   - Postgres schema (conversations + messages tables)
   - Database connection pooling (asyncpg)
   - Save messages during streaming
   - GET /conversation/{id} endpoint
   - App startup/shutdown lifecycle hooks
   - **Checkpoint**: Verify messages persist, history retrieved correctly

5. **Phase 5: Deployment & Docker (P1)** - 12 min
   - Backend Dockerfile (Python 3.11-slim)
   - docker-compose.yml (backend + frontend services)
   - Environment variable configuration
   - Production deployment configs (Vercel + backend host)
   - **Checkpoint**: Verify Docker Compose starts, full flow works, production build succeeds

6. **Phase 6: Testing & Evaluation (P1)** - 14 min
   - Backend unit tests (embedding service)
   - Integration tests (chat + health endpoints)
   - Golden evaluation dataset (20 domain + 5 out-of-scope questions)
   - Evaluation script (SC-012: ‚â•90% accuracy at >0.7 similarity)
   - Coverage reporting (target: ‚â•80% backend)
   - **Checkpoint**: All tests pass, evaluation ‚â•90%, coverage meets target

**Total Estimated Time**: ~100 minutes of focused implementation (excluding deployment/testing time)

**Key Technical Details**:
- **Chunking Strategy**: Semantic boundaries at H2/H3 headings, preserve parent hierarchy, max 512 tokens per chunk
- **Embedding Dimensions**: 768 (Gemini text-embedding-004)
- **Similarity Metric**: Cosine (best for normalized embeddings)
- **Streaming Protocol**: SSE with JSON-encoded events (`data: {"type":"token","content":"..."}\n\n`)
- **CORS Origins**: `http://localhost:3000` (dev), `https://ai-humanoid-robot.vercel.app` (prod)
- **Rate Limits**: Gemini free tier = 10 RPM (requests per minute) for gemini-2.0-flash-exp

**Success Criteria Met**:
- ‚úÖ SC-001: Streaming response < 1s first token, < 10s complete (p95)
- ‚úÖ SC-003: 100% valid markdown processed (error handling for invalid)
- ‚úÖ SC-004: ‚â•90% accuracy at >0.7 similarity (evaluation script validates)
- ‚úÖ SC-008: Docker deployment with `docker-compose up`
- ‚úÖ SC-011: `npm run build` succeeds
- ‚úÖ SC-012: Evaluation dataset passes

**Post-MVP Enhancements** (deferred):
- Advanced chunking experiments (256/512/1024 tokens + overlap)
- Redis caching for frequent queries
- Multi-turn context (last 3 Q&A pairs in general mode)
- Frontend component tests (Jest + RTL)
- CI/CD pipeline (GitHub Actions)
- Rate limiting middleware
- User feedback collection (thumbs up/down)

## Outcome

- ‚úÖ Impact: Complete implementation plan with 6 phases, 40+ micro-tasks, clear checkpoints and verification commands. Ready for execution.
- üß™ Tests: Evaluation framework defined (Phase 6), target ‚â•90% accuracy, ‚â•80% coverage
- üìÅ Files: 1 created (specs/rag-chatbot/plan.md - 600+ lines)
- üîÅ Next prompts: Run `/sp.tasks` to generate actionable task list with dependencies
- üß† Reflection: Plan balances MVP speed with quality checkpoints. Each phase has clear verification criteria. Architectural decisions documented for future reference. Gemini integration via OpenAI SDK reduces implementation risk.

## Evaluation notes (flywheel)

- Failure modes observed: None. Plan covers all P1 requirements from spec, defers P3 appropriately.
- Graders run and results (PASS/FAIL): PASS - All FRs addressed, phased approach follows user priorities, checkpoints ensure quality gates
- Prompt variant (if applicable): Standard /sp.plan with user-specified micro-task sizing (2-5 min)
- Next experiment (smallest change to try): During Task phase, measure actual task completion times to calibrate 2-5 min estimates for future planning
